{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 13:12:38.029483: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 13:12:38.417718: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-16 13:12:40.479872: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 13:12:40.480178: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 13:12:40.480193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from LRFutils import archive, progress\n",
    "import os\n",
    "from multiprocess import Pool\n",
    "import psutil\n",
    "\n",
    "archive_path = archive.new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 16 13:12:43 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   23C    P8    12W / 250W |      0MiB / 22698MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one vector, we expect something like this:\n",
    "\n",
    "**Input $x_i$:**\n",
    "- I[x,y,f] : data cube of intensity for a given pixel (x,y) and frequency f\n",
    "\n",
    "**Outputs $y_i$:**\n",
    "- Vx[x,y,z] : data cube of velocity in x direction for a given coordinate in space (x,y,z)\n",
    "- Vy[x,y,z] : data cube of velocity in y direction for a given coordinate in space (x,y,z)\n",
    "- Vz[x,y,z] : data cube of velocity in z direction for a given coordinate in space (x,y,z)\n",
    "- $\\rho$[x,y,z] : data cube of density for a given coordinate in space (x,y,z)\n",
    "\n",
    "In practice, there is lot of vectors, so $x = [x_1, x_2, ..., x_N]$ and $y = [y_1, y_2, ..., y_N]$ where $N$ is the number of vectors.\n",
    "\n",
    "To simplify the neural network and potentially increase it's accuracy, we will not design a network that predicts all of the outputs at once. Instead, we will design 4 networks that will predict one output. This means that we will have 4 networks, one for each output. So $y_i$ will alternatively contain only one of the elements listed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **YOUR JOB**: In the following cell, write the code that load the data as specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file):\n",
    "    data = np.load(\"dataset/\" + file)\n",
    "    return data\n",
    "\n",
    "def load_data() -> tuple[np.ndarray, tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Do what you want int this function, as long as it returns the following:\n",
    "    - list[3D-ndarray] : input vectors\n",
    "    - list[3D-ndarray] : output vectors\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y0 = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    y3 = []\n",
    "\n",
    "    window_size = 5\n",
    "\n",
    "    width = (window_size-1)//2\n",
    "\n",
    "    for file in os.listdir(\"dataset/\")[0:2]:\n",
    "        data = np.load(\"dataset/\" + file)\n",
    "        crop = np.arange(width, data[\"observation\"].shape[1]-width)\n",
    "        \n",
    "        bar = progress.Bar(len(crop)**2, prefix=f\"{file}, {len(x)}\")\n",
    "        for n, i in enumerate(crop):\n",
    "            for m, j in enumerate(crop):\n",
    "                \n",
    "                prefix = f\"CPU: {psutil.cpu_percent()}%, RAM: {psutil.virtual_memory().percent}% ({psutil.virtual_memory().used/1024**3:.2f}GB / {psutil.virtual_memory().total/1024**3:.2f}GB)\"\n",
    "                \n",
    "                bar(n*len(crop)+m, prefix=f\"{file} | {prefix}\")\n",
    "                # print(\"------\")\n",
    "                # print(file,i,j)\n",
    "                x.append(data[\"observation\"][i-width:i+width+1, j-width:j+width+1, :])\n",
    "                # print(\"x\",x[-1].shape)\n",
    "                y0.append(data[\"cloud\"][i, j, :])\n",
    "                # print(\"y0\",y0[-1].shape)\n",
    "                y1.append(data[\"vx\"][i, j, :])\n",
    "                # print(\"y1\",y1[-1].shape)\n",
    "                y2.append(data[\"vy\"][i, j, :])\n",
    "                # print(\"y2\",y2[-1].shape)\n",
    "                y3.append(data[\"vz\"][i, j, :])\n",
    "                # print(\"y3\",y3[-1].shape)\n",
    "    \n",
    "    bar(len(crop))\n",
    "\n",
    "    print(\"x shape: \", np.array(x).shape)\n",
    "    \n",
    "    return x, y0, y1, y2, y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Post data treatment\n",
    "\n",
    "This part only consist to check the data consistency, normalize it and split the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification that the data are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mcloud_0_obs_2.npz | CPU: 2.8%, RAM: 32.9% (60.34GB / 187.54GB) \u001b[92m━━\u001b[37m╺\u001b[37m━━━━━━━ \u001b[92m21% \u001b[91m777/3600 \u001b[35m0:00:45 \u001b[0meta \u001b[34m0:02:44\u001b[0mm\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x, y0, y1, y2, y3 \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y0) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y1) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y2) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y3), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have the same length, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(y0)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(y1)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(y2)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(y3)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x)\n",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             y2\u001b[38;5;241m.\u001b[39mappend(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvy\u001b[39m\u001b[38;5;124m\"\u001b[39m][i, j, :])\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# print(\"y2\",y2[-1].shape)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m             y3\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[i, j, :])\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# print(\"y3\",y3[-1].shape)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m bar(\u001b[38;5;28mlen\u001b[39m(crop))\n",
      "File \u001b[0;32m~/M2-Prestel-state-from-obs-ML/venv/lib/python3.9/site-packages/numpy/lib/npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/M2-Prestel-state-from-obs-ML/venv/lib/python3.9/site-packages/numpy/lib/format.py:823\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    821\u001b[0m             read_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_read_count, count \u001b[38;5;241m-\u001b[39m i)\n\u001b[1;32m    822\u001b[0m             read_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(read_count \u001b[38;5;241m*\u001b[39m dtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[0;32m--> 823\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m             array[i:i\u001b[38;5;241m+\u001b[39mread_count] \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfrombuffer(data, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    825\u001b[0m                                                      count\u001b[38;5;241m=\u001b[39mread_count)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m~/M2-Prestel-state-from-obs-ML/venv/lib/python3.9/site-packages/numpy/lib/format.py:958\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 958\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m size:\n",
      "File \u001b[0;32m/usr/lib/python3.9/zipfile.py:922\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 922\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m    924\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/usr/lib/python3.9/zipfile.py:1012\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_crc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/lib/python3.9/zipfile.py:937\u001b[0m, in \u001b[0;36mZipExtFile._update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expected_crc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;66;03m# No need to compute the CRC if we don't have a reference value\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_crc \u001b[38;5;241m=\u001b[39m \u001b[43mcrc32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_running_crc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Check the CRC if we're at the end of the file\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_crc \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expected_crc:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x, y0, y1, y2, y3 = load_data()\n",
    "assert len(x) == len(y0) == len(y1) == len(y2) == len(y3), f\"x and y must have the same length, found {len(x)}, {len(y0)}, {len(y1)}, {len(y2)}, {len(y3)}\"\n",
    "x = np.array(x)\n",
    "y0 = np.array(y0)\n",
    "y1 = np.array(y1)\n",
    "y2 = np.array(y2)\n",
    "y3 = np.array(y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_vectors = len(x)\n",
    "size_x = x[0].shape\n",
    "size_x_x, size_x_y, size_x_z = size_x\n",
    "channel_x = 1\n",
    "input_shape = (size_x_x, size_x_y, size_x_z, channel_x)\n",
    "\n",
    "size_y0 = y0[0].shape\n",
    "size_y0_x, size_y0_y, size_y0_z = size_y0\n",
    "channel_y0 = 1\n",
    "output0_shape = (size_y0_x, size_y0_y, size_y0_z, channel_y0)\n",
    "\n",
    "size_y1 = y1[0].shape\n",
    "size_y1_x, size_y1_y, size_y1_z = size_y1\n",
    "channel_y1 = 1\n",
    "output1_shape = (size_y1_x, size_y1_y, size_y1_z, channel_y1)\n",
    "\n",
    "size_y2 = y2[0].shape\n",
    "size_y2_x, size_y2_y, size_y2_z = size_y2\n",
    "channel_y2 = 1\n",
    "output2_shape = (size_y2_x, size_y2_y, size_y2_z, channel_y2)\n",
    "\n",
    "size_y3 = y3[0].shape\n",
    "size_y3_x, size_y3_y, size_y3_z = size_y3\n",
    "channel_y3 = 1\n",
    "output3_shape = (size_y3_x, size_y3_y, size_y3_z, channel_y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x /= np.max(np.abs(x))\n",
    "y0 /= np.max(np.abs(y0))\n",
    "y1 /= np.max(np.abs(y1))\n",
    "y2 /= np.max(np.abs(y2))\n",
    "y3 /= np.max(np.abs(y3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_frac=0.2\n",
    "test_frac=0.1\n",
    "\n",
    "train_frac = 1 - valid_frac - test_frac\n",
    "\n",
    "train_x = x[:int(nb_vectors * train_frac)]\n",
    "train_y0 = y0[:int(nb_vectors * train_frac)]\n",
    "train_y1 = y1[:int(nb_vectors * train_frac)]\n",
    "train_y2 = y2[:int(nb_vectors * train_frac)]\n",
    "train_y3 = y3[:int(nb_vectors * train_frac)]\n",
    "train_y = [train_y0, train_y1, train_y2, train_y3]\n",
    "\n",
    "valid_x = x[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "valid_y0 = y0[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "valid_y1 = y1[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "valid_y2 = y2[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "valid_y3 = y3[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "valid_y = [valid_y0, valid_y1, valid_y2, valid_y3]\n",
    "\n",
    "test_x = x[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "test_y0 = y0[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "test_y1 = y1[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "test_y2 = y2[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "test_y3 = y3[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "test_y = [test_y0, test_y1, test_y2, test_y3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the 3D CNN model\n",
    "\n",
    "def get_model(input_shape, output_shape):\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv3D(32, kernel_size=(15, 15, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n",
    "    # > Utile ?\n",
    "    # model.add(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(tf.keras.layers.Conv3D(64, kernel_size=(5, 5, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    # model.add(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    # <\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(np.prod(output_shape), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(tf.keras.layers.Dense(np.prod(output_shape), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(tf.keras.layers.Dense(np.prod(output_shape), activation='relu', kernel_initializer='he_uniform'))\n",
    "    # > Utile ?\n",
    "    # model.add(tf.keras.layers.Dense(np.prod(output_shape), activation='softmax'))\n",
    "    # <\n",
    "    model.add(tf.keras.layers.Reshape(output_shape))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model for y0\n",
    "\n",
    "model0 = get_model(input_shape, output0_shape)\n",
    "model0.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model0.fit(train_x, train_y0, epochs=10, batch_size=32, validation_data=(valid_x, valid_y0))\n",
    "model0.save(f'{archive_path}/model0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model for y1\n",
    "\n",
    "model1 = get_model(input_shape, output0_shape)\n",
    "model1.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model1.fit(train_x, train_y1, epochs=10, batch_size=32, validation_data=(valid_x, valid_y1))\n",
    "model1.save(f'{archive_path}/model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model for y2\n",
    "\n",
    "model2 = get_model(input_shape, output0_shape)\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model2.fit(train_x, train_y2, epochs=10, batch_size=32, validation_data=(valid_x, valid_y2))\n",
    "model2.save(f'{archive_path}/model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model for y3\n",
    "\n",
    "model3 = get_model(input_shape, output0_shape)\n",
    "model3.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model3.fit(train_x, train_y3, epochs=10, batch_size=32, validation_data=(valid_x, valid_y3))\n",
    "model3.save(f'{archive_path}/model3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model for y0\n",
    "\n",
    "score0 = model0.evaluate(test_x, test_y0, verbose=0)\n",
    "print('Test loss:', score0[0])\n",
    "print('Test accuracy:', score0[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model for y1\n",
    "\n",
    "score1 = model1.evaluate(test_x, test_y1, verbose=0)\n",
    "print('Test loss:', score1[0])\n",
    "print('Test accuracy:', score1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model for y2\n",
    "\n",
    "score2 = model2.evaluate(test_x, test_y2, verbose=0)\n",
    "print('Test loss:', score2[0])\n",
    "print('Test accuracy:', score2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model for y3\n",
    "\n",
    "score3 = model3.evaluate(test_x, test_y3, verbose=0)\n",
    "print('Test loss:', score3[0])\n",
    "print('Test accuracy:', score3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{archive_path}/scores.txt', 'w') as f:\n",
    "    f.write('\\t\\t\\t\\tModel 0\\tModel 1\\tModel 2\\tModel 3\\n')\n",
    "    f.write(f'Test loss:    \\t{round(score0[0],3)}\\t{round(score1[0],3)}\\t{round(score2[0],3)}\\t{round(score3[0],3)}\\n')\n",
    "    f.write(f'Test accuracy:\\t{round(score0[1],3)}\\t{round(score1[1],3)}\\t{round(score2[1],3)}\\t{round(score3[1],3)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "x_prediction = [x[0,...]]\n",
    "x_prediction = np.expand_dims(x_prediction, axis=-1)\n",
    "print(x_prediction.shape)\n",
    "\n",
    "y0_prediction = model0.predict(x_prediction)\n",
    "print(y0_prediction.shape)\n",
    "\n",
    "# y1_prediction = model1.predict([x_prediction])\n",
    "# print(y1_prediction.shape)\n",
    "\n",
    "# y2_prediction = model2.predict([x_prediction])\n",
    "# print(y2_prediction.shape)\n",
    "\n",
    "# y3_prediction = model3.predict([x_prediction])\n",
    "# print(y3_prediction.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f3504165b11491f2f78d69ba41d30335901f7599302f48b3bfea9c0b8f9ae61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
