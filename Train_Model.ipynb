{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from LRFutils import archive, progress\n",
    "import os\n",
    "\n",
    "archive_path = archive.new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because you are not:\n",
      "\ta) running as an administrator or\n",
      "\tb) there is not at least one TCC device in the system\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one vector, we expect something like this:\n",
    "\n",
    "**Input $x_i$:**\n",
    "- I[x,y,f] : data cube of intensity for a given pixel (x,y) and frequency f\n",
    "\n",
    "**Outputs $y_i$:**\n",
    "- Vx[x,y,z] : data cube of velocity in x direction for a given coordinate in space (x,y,z)\n",
    "- Vy[x,y,z] : data cube of velocity in y direction for a given coordinate in space (x,y,z)\n",
    "- Vz[x,y,z] : data cube of velocity in z direction for a given coordinate in space (x,y,z)\n",
    "- $\\rho$[x,y,z] : data cube of density for a given coordinate in space (x,y,z)\n",
    "\n",
    "In practice, there is lot of vectors, so $x = [x_1, x_2, ..., x_N]$ and $y = [y_1, y_2, ..., y_N]$ where $N$ is the number of vectors.\n",
    "\n",
    "To simplify the neural network and potentially increase it's accuracy, we will not design a network that predicts all of the outputs at once. Instead, we will design 4 networks that will predict one output. This means that we will have 4 networks, one for each output. So $y_i$ will alternatively contain only one of the elements listed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **YOUR JOB**: In the following cell, write the code that load the data as specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> tuple[np.ndarray, tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Do what you want int this function, as long as it returns the following:\n",
    "    - list[3D-ndarray] : input vectors\n",
    "    - list[3D-ndarray] : output vectors\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y0 = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    y3 = []\n",
    "\n",
    "    window_size = 5\n",
    "\n",
    "    width = (window_size-1)//2\n",
    "\n",
    "    for file in os.listdir(\"dataset/\")[0:2]:\n",
    "        data = np.load(\"dataset/\" + file)\n",
    "        crop = np.arange(width, data[\"observation\"].shape[1]-width)\n",
    "        \n",
    "        bar = progress.Bar(len(crop)**2, prefix=f\"{file}, {len(x)}\")\n",
    "        for n, i in enumerate(crop):\n",
    "            for m, j in enumerate(crop):\n",
    "                bar(n*len(crop)+m, prefix=f\"{file}, {len(x)}\")\n",
    "                # print(\"------\")\n",
    "                # print(file,i,j)\n",
    "                x.append(data[\"observation\"][i-width:i+width+1, j-width:j+width+1, :])\n",
    "                # print(\"x\",x[-1].shape)\n",
    "                y0.append(data[\"cloud\"][i, j, :])\n",
    "                # print(\"y0\",y0[-1].shape)\n",
    "                y1.append(data[\"vx\"][i, j, :])\n",
    "                # print(\"y1\",y1[-1].shape)\n",
    "                y2.append(data[\"vy\"][i, j, :])\n",
    "                # print(\"y2\",y2[-1].shape)\n",
    "                y3.append(data[\"vz\"][i, j, :])\n",
    "                # print(\"y3\",y3[-1].shape)\n",
    "    \n",
    "    bar(len(crop))\n",
    "\n",
    "    print(\"x shape: \", np.array(x).shape)\n",
    "    \n",
    "    return x, y0, y1, y2, y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Post data treatment\n",
    "\n",
    "This part only consist to check the data consistency, normalize it and split the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification that the data are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mcloud_0_obs_0.npz, 252 \u001b[92m━\u001b[92m╸\u001b[37m━━━━━━━━━━━━━━━━━━━━━━━ \u001b[92m7% \u001b[91m252/3600 \u001b[35m0:00:29 \u001b[0meta \u001b[34m0:08:49\u001b[0m\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x, y0, y1, y2, y3 \u001b[39m=\u001b[39m load_data()\n\u001b[0;32m      2\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(x) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(y0) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(y1) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(y2) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(y3), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have the same length, found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(y0)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(y1)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(y2)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(y3)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(x)\n",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m y0\u001b[39m.\u001b[39mappend(data[\u001b[39m\"\u001b[39m\u001b[39mcloud\u001b[39m\u001b[39m\"\u001b[39m][i, j, :])\n\u001b[0;32m     29\u001b[0m \u001b[39m# print(\"y0\",y0[-1].shape)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m y1\u001b[39m.\u001b[39mappend(data[\u001b[39m\"\u001b[39;49m\u001b[39mvx\u001b[39;49m\u001b[39m\"\u001b[39;49m][i, j, :])\n\u001b[0;32m     31\u001b[0m \u001b[39m# print(\"y1\",y1[-1].shape)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m y2\u001b[39m.\u001b[39mappend(data[\u001b[39m\"\u001b[39m\u001b[39mvy\u001b[39m\u001b[39m\"\u001b[39m][i, j, :])\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Dev\\CompuPhys\\M2-Prestel-state-from-obs-ML\\venv\\lib\\site-packages\\numpy\\lib\\npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n\u001b[0;32m    252\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[1;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(\u001b[39mbytes\u001b[39;49m,\n\u001b[0;32m    254\u001b[0m                              allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallow_pickle,\n\u001b[0;32m    255\u001b[0m                              pickle_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpickle_kwargs,\n\u001b[0;32m    256\u001b[0m                              max_header_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_header_size)\n\u001b[0;32m    257\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Dev\\CompuPhys\\M2-Prestel-state-from-obs-ML\\venv\\lib\\site-packages\\numpy\\lib\\format.py:823\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    821\u001b[0m             read_count \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(max_read_count, count \u001b[39m-\u001b[39m i)\n\u001b[0;32m    822\u001b[0m             read_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(read_count \u001b[39m*\u001b[39m dtype\u001b[39m.\u001b[39mitemsize)\n\u001b[1;32m--> 823\u001b[0m             data \u001b[39m=\u001b[39m _read_bytes(fp, read_size, \u001b[39m\"\u001b[39;49m\u001b[39marray data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    824\u001b[0m             array[i:i\u001b[39m+\u001b[39mread_count] \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mfrombuffer(data, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m    825\u001b[0m                                                      count\u001b[39m=\u001b[39mread_count)\n\u001b[0;32m    827\u001b[0m \u001b[39mif\u001b[39;00m fortran_order:\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\Dev\\CompuPhys\\M2-Prestel-state-from-obs-ML\\venv\\lib\\site-packages\\numpy\\lib\\format.py:958\u001b[0m, in \u001b[0;36m_read_bytes\u001b[1;34m(fp, size, error_template)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    954\u001b[0m     \u001b[39m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[0;32m    955\u001b[0m     \u001b[39m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[0;32m    956\u001b[0m     \u001b[39m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[0;32m    957\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 958\u001b[0m         r \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(size \u001b[39m-\u001b[39;49m \u001b[39mlen\u001b[39;49m(data))\n\u001b[0;32m    959\u001b[0m         data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[0;32m    960\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(r) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m size:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\\lib\\zipfile.py:925\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    924\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[1;32m--> 925\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[0;32m    926\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[0;32m    927\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\\lib\\zipfile.py:1001\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_type \u001b[39m==\u001b[39m ZIP_DEFLATED:\n\u001b[0;32m   1000\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMIN_READ_SIZE)\n\u001b[1;32m-> 1001\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(data, n)\n\u001b[0;32m   1002\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39meof \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m                  \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n\u001b[0;32m   1005\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x, y0, y1, y2, y3 = load_data()\n",
    "assert len(x) == len(y0) == len(y1) == len(y2) == len(y3), f\"x and y must have the same length, found {len(x)}, {len(y0)}, {len(y1)}, {len(y2)}, {len(y3)}\"\n",
    "x = np.array(x)\n",
    "y0 = np.array(y0)\n",
    "y1 = np.array(y1)\n",
    "y2 = np.array(y2)\n",
    "y3 = np.array(y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_vectors = len(x)\n",
    "size_x = x[0].shape\n",
    "size_x_x, size_x_y, size_x_z = size_x\n",
    "channel_x = 1\n",
    "input_shape = (size_x_x, size_x_y, size_x_z, channel_x)\n",
    "\n",
    "size_y0 = y0[0].shape\n",
    "size_y0_x, size_y0_y, size_y0_z = size_y0\n",
    "channel_y0 = 1\n",
    "output0_shape = (size_y0_x, size_y0_y, size_y0_z, channel_y0)\n",
    "\n",
    "size_y1 = y1[0].shape\n",
    "size_y1_x, size_y1_y, size_y1_z = size_y1\n",
    "channel_y1 = 1\n",
    "output1_shape = (size_y1_x, size_y1_y, size_y1_z, channel_y1)\n",
    "\n",
    "size_y2 = y2[0].shape\n",
    "size_y2_x, size_y2_y, size_y2_z = size_y2\n",
    "channel_y2 = 1\n",
    "output2_shape = (size_y2_x, size_y2_y, size_y2_z, channel_y2)\n",
    "\n",
    "size_y3 = y3[0].shape\n",
    "size_y3_x, size_y3_y, size_y3_z = size_y3\n",
    "channel_y3 = 1\n",
    "output3_shape = (size_y3_x, size_y3_y, size_y3_z, channel_y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x /= np.max(np.abs(x))\n",
    "y0 /= np.max(np.abs(y0))\n",
    "y1 /= np.max(np.abs(y1))\n",
    "y2 /= np.max(np.abs(y2))\n",
    "y3 /= np.max(np.abs(y3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_frac=0.2\n",
    "test_frac=0.1\n",
    "\n",
    "train_frac = 1 - valid_frac - test_frac\n",
    "\n",
    "train_x = x[:int(nb_vectors * train_frac)]\n",
    "train_y0 = y0[:int(nb_vectors * train_frac)]\n",
    "train_y1 = y1[:int(nb_vectors * train_frac)]\n",
    "train_y2 = y2[:int(nb_vectors * train_frac)]\n",
    "train_y3 = y3[:int(nb_vectors * train_frac)]\n",
    "train_y = [train_y0, train_y1, train_y2, train_y3]\n",
    "\n",
    "valid_x = x[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "valid_y0 = y0[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "valid_y1 = y1[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "valid_y2 = y2[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "valid_y3 = y3[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "valid_y = [valid_y0, valid_y1, valid_y2, valid_y3]\n",
    "\n",
    "test_x = x[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "test_y0 = y0[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "test_y1 = y1[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "test_y2 = y2[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "test_y3 = y3[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "test_y = [test_y0, test_y1, test_y2, test_y3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the 3D CNN model\n",
    "\n",
    "def get_model(input_shape, output_shape):\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv3D(32, kernel_size=(15, 15, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape))\n",
    "    # > Utile ?\n",
    "    # model.add(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(tf.keras.layers.Conv3D(64, kernel_size=(5, 5, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    # model.add(tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    # <\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(np.prod(output_shape), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(tf.keras.layers.Dense(np.prod(output_shape), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(tf.keras.layers.Dense(np.prod(output_shape), activation='relu', kernel_initializer='he_uniform'))\n",
    "    # > Utile ?\n",
    "    # model.add(tf.keras.layers.Dense(np.prod(output_shape), activation='softmax'))\n",
    "    # <\n",
    "    model.add(tf.keras.layers.Reshape(output_shape))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model for y0\n",
    "\n",
    "model0 = get_model(input_shape, output0_shape)\n",
    "model0.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model0.fit(train_x, train_y0, epochs=10, batch_size=32, validation_data=(valid_x, valid_y0))\n",
    "model0.save(f'{archive_path}/model0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model for y1\n",
    "\n",
    "model1 = get_model(input_shape, output0_shape)\n",
    "model1.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model1.fit(train_x, train_y1, epochs=10, batch_size=32, validation_data=(valid_x, valid_y1))\n",
    "model1.save(f'{archive_path}/model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model for y2\n",
    "\n",
    "model2 = get_model(input_shape, output0_shape)\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model2.fit(train_x, train_y2, epochs=10, batch_size=32, validation_data=(valid_x, valid_y2))\n",
    "model2.save(f'{archive_path}/model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model for y3\n",
    "\n",
    "model3 = get_model(input_shape, output0_shape)\n",
    "model3.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model3.fit(train_x, train_y3, epochs=10, batch_size=32, validation_data=(valid_x, valid_y3))\n",
    "model3.save(f'{archive_path}/model3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model for y0\n",
    "\n",
    "score0 = model0.evaluate(test_x, test_y0, verbose=0)\n",
    "print('Test loss:', score0[0])\n",
    "print('Test accuracy:', score0[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model for y1\n",
    "\n",
    "score1 = model1.evaluate(test_x, test_y1, verbose=0)\n",
    "print('Test loss:', score1[0])\n",
    "print('Test accuracy:', score1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model for y2\n",
    "\n",
    "score2 = model2.evaluate(test_x, test_y2, verbose=0)\n",
    "print('Test loss:', score2[0])\n",
    "print('Test accuracy:', score2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model for y3\n",
    "\n",
    "score3 = model3.evaluate(test_x, test_y3, verbose=0)\n",
    "print('Test loss:', score3[0])\n",
    "print('Test accuracy:', score3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{archive_path}/scores.txt', 'w') as f:\n",
    "    f.write('\\t\\t\\t\\tModel 0\\tModel 1\\tModel 2\\tModel 3\\n')\n",
    "    f.write(f'Test loss:    \\t{round(score0[0],3)}\\t{round(score1[0],3)}\\t{round(score2[0],3)}\\t{round(score3[0],3)}\\n')\n",
    "    f.write(f'Test accuracy:\\t{round(score0[1],3)}\\t{round(score1[1],3)}\\t{round(score2[1],3)}\\t{round(score3[1],3)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "x_prediction = [x[0,...]]\n",
    "x_prediction = np.expand_dims(x_prediction, axis=-1)\n",
    "print(x_prediction.shape)\n",
    "\n",
    "y0_prediction = model0.predict(x_prediction)\n",
    "print(y0_prediction.shape)\n",
    "\n",
    "# y1_prediction = model1.predict([x_prediction])\n",
    "# print(y1_prediction.shape)\n",
    "\n",
    "# y2_prediction = model2.predict([x_prediction])\n",
    "# print(y2_prediction.shape)\n",
    "\n",
    "# y3_prediction = model3.predict([x_prediction])\n",
    "# print(y3_prediction.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4273a5f9333d8b877e3d7dd39ae42051c41fece7153122659cbd873aee3ad249"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
