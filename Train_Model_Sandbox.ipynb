{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# **Initialisation**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 30 09:52:05 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   30C    P8    21W / 250W |      0MiB / 22698MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvcc: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF config\n",
    "\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 09:52:06.892777: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 09:52:08.359749: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64\n",
      "2023-03-30 09:52:08.359942: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64\n",
      "2023-03-30 09:52:08.359947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from LRFutils import archive, progress\n",
    "from multiprocess import Pool, cpu_count\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy as copy\n",
    "import pickle\n",
    "from time import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit the config file to match your needs\n",
    "\n",
    "valid_frac = 0.2\n",
    "test_frac  = 0.1\n",
    "dataset_path = \"data/dataset\"\n",
    "epochs = 1000\n",
    "batch_size=100\n",
    "loss = 'mean_squared_error'\n",
    "optimizer = 'SGD'\n",
    "metrics = [\n",
    "    tf.keras.metrics.MeanAbsoluteError(name=\"MAE\"),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive created at ./archives/2023-03-30_62d600f/1\n"
     ]
    }
   ],
   "source": [
    "# Global variable\n",
    "\n",
    "archive_path = archive.new(verbose=True)\n",
    "\n",
    "try:\n",
    "    ncpu = cpu_count()\n",
    "except:\n",
    "    with open(os.getenv(\"OAR_NODEFILE\"), 'r') as f:\n",
    "        ncpu = len(f.readlines())\n",
    "\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functions\n",
    "\n",
    "def system_info():\n",
    "    return f\"CPU: {psutil.cpu_percent()}%\"\\\n",
    "        + f\", RAM: {psutil.virtual_memory().percent}%\"\\\n",
    "        + f\" ({psutil.virtual_memory().used/1024**3:.2f}GB\"\\\n",
    "        + f\"/ {psutil.virtual_memory().total/1024**3:.2f}GB)\"\n",
    "\n",
    "def nb_vec(x:dict) -> int:\n",
    "\n",
    "    for key in x.keys():\n",
    "        assert len(x[key]) == len(x[list(x.keys())[0]]), \"All dictionary element must be the same length\"\n",
    "\n",
    "    return len(x[list(x.keys())[0]])\n",
    "\n",
    "def get_sample(x):\n",
    "    sample = {}\n",
    "    for key, value in x.items():\n",
    "        sample[key] = value[0]\n",
    "    return sample\n",
    "\n",
    "def shuffle(x, y):\n",
    "\n",
    "    nb_vectors = nb_vec(x)\n",
    "    assert nb_vec(y) == nb_vectors, \"x and y must have the same number of vectors\"\n",
    "\n",
    "    idx = np.random.permutation(nb_vectors)\n",
    "\n",
    "    x_copy = copy(x)\n",
    "    for key in x.keys():\n",
    "        x_copy[key] = x[key][idx]\n",
    "\n",
    "    y_copy = copy(y)\n",
    "    for key in y.keys():\n",
    "        y_copy[key] = y[key][idx]\n",
    "\n",
    "    return x_copy, y_copy\n",
    "\n",
    "def pick_vec(x, y, idx):\n",
    "    vec_x = {}\n",
    "    for key, value in x.items():\n",
    "        vec_x[key] = value[idx]\n",
    "    \n",
    "    vec_y = {}\n",
    "    for key, value in y.items():\n",
    "        vec_y[key] = value[idx]\n",
    "\n",
    "    return vec_x, vec_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# **Load data**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit the data loading to match your needs\n",
    "\n",
    "def load_file(file):\n",
    "\n",
    "    data = np.load(file)\n",
    "\n",
    "    x = {\n",
    "        # \"Dust wavelenght\": np.array([250.,]), # dust observation frequency [um]\n",
    "        \"Dust map\" : data[\"dust_image\"].reshape(*data[\"dust_image\"].shape, 1), # adding a channel dimension\n",
    "        # \"CO velocity\" : data[\"CO_v\"],\n",
    "        \"CO cube\" : data[\"CO_cube\"].reshape(*data[\"CO_cube\"].shape, 1), # adding a channel dimension\n",
    "        # \"N2H+ velocity\" : data[\"N2H_v\"],\n",
    "        # \"N2H cube\" : data[\"N2H_cube\"].reshape(*data[\"N2H_cube\"].shape, 1), # adding a channel dimension\n",
    "    }\n",
    "    \n",
    "    y = {\n",
    "        \"Total mass\" : np.array(data[\"mass\"]),\n",
    "        \"Max temperature\" : np.array(np.amax(data[\"dust_temperature\"])),\n",
    "    }\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over files\n",
    "\n",
    "def load_data() -> tuple[np.ndarray, tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Do what you want int this function, as long as it returns the following:\n",
    "    - list[3D-ndarray] : input vectors\n",
    "    - list[3D-ndarray] : output vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Limit of the number of vectors to load\n",
    "    max_files = 1000\n",
    "    files = os.listdir(dataset_path)\n",
    "    nb_vectors = min(len(files), max_files)\n",
    "\n",
    "    # Load data\n",
    "    x = {}\n",
    "    y = {}\n",
    "    bar = progress.Bar(nb_vectors, \"Loading data\")\n",
    "    for i, file in enumerate(files):\n",
    "        if i >= nb_vectors:\n",
    "            break\n",
    "        bar(i, prefix=system_info())\n",
    "        \n",
    "        new_x, new_y = load_file(f\"{dataset_path}/{file}\")\n",
    "\n",
    "        for key, value in new_x.items():\n",
    "            if key not in x:\n",
    "                x[key] = []\n",
    "            x[key].append(value)\n",
    "\n",
    "        for key, value in new_y.items():\n",
    "            if key not in y:\n",
    "                y[key] = []\n",
    "            y[key].append(value)\n",
    "\n",
    "    \n",
    "    for key in x.keys():\n",
    "        x[key] = np.array(x[key])\n",
    "    \n",
    "    for key in y.keys():\n",
    "        y[key] = np.array(y[key])\n",
    "        \n",
    "    bar(nb_vectors)    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mCPU: 1.2%, RAM: 3.7% (5.53GB/ 187.54GB) \u001b[92m━\u001b[92m╸\u001b[37m━━━━━━━━ \u001b[92m18% \u001b[91m182/1000 \u001b[35m0:00:04 \u001b[0meta \u001b[34m0:00:18\u001b[0m\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m x, y \u001b[39m=\u001b[39m load_data()\n\u001b[1;32m      4\u001b[0m nb_vectors \u001b[39m=\u001b[39m nb_vec(x)\n\u001b[1;32m      5\u001b[0m x_labels \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(x\u001b[39m.\u001b[39mkeys())\n",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     21\u001b[0m bar(i, prefix\u001b[39m=\u001b[39msystem_info())\n\u001b[0;32m---> 23\u001b[0m new_x, new_y \u001b[39m=\u001b[39m load_file(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mdataset_path\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mfile\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m new_x\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m x:\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mload_file\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_file\u001b[39m(file):\n\u001b[1;32m      5\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(file)\n\u001b[1;32m      7\u001b[0m     x \u001b[39m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m         \u001b[39m# \"Dust wavelenght\": np.array([250.,]), # dust observation frequency [um]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDust map\u001b[39m\u001b[39m\"\u001b[39m : data[\u001b[39m\"\u001b[39m\u001b[39mdust_image\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mdust_image\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape, \u001b[39m1\u001b[39m), \u001b[39m# adding a channel dimension\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[39m# \"CO velocity\" : data[\"CO_v\"],\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCO cube\u001b[39m\u001b[39m\"\u001b[39m : data[\u001b[39m\"\u001b[39;49m\u001b[39mCO_cube\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mCO_cube\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape, \u001b[39m1\u001b[39m), \u001b[39m# adding a channel dimension\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[39m# \"N2H+ velocity\" : data[\"N2H_v\"],\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[39m# \"N2H cube\" : data[\"N2H_cube\"].reshape(*data[\"N2H_cube\"].shape, 1), # adding a channel dimension\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     }\n\u001b[1;32m     16\u001b[0m     y \u001b[39m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTotal mass\u001b[39m\u001b[39m\"\u001b[39m : np\u001b[39m.\u001b[39marray(data[\u001b[39m\"\u001b[39m\u001b[39mmass\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m     18\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMax temperature\u001b[39m\u001b[39m\"\u001b[39m : np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39mamax(data[\u001b[39m\"\u001b[39m\u001b[39mdust_temperature\u001b[39m\u001b[39m\"\u001b[39m])),\n\u001b[1;32m     19\u001b[0m     }\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m x, y\n",
      "File \u001b[0;32m~/M2-Prestel-state-from-obs-ML/venv/lib/python3.9/site-packages/numpy/lib/npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    252\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(\u001b[39mbytes\u001b[39;49m,\n\u001b[1;32m    254\u001b[0m                              allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallow_pickle,\n\u001b[1;32m    255\u001b[0m                              pickle_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    256\u001b[0m                              max_header_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_header_size)\n\u001b[1;32m    257\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/M2-Prestel-state-from-obs-ML/venv/lib/python3.9/site-packages/numpy/lib/format.py:812\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    810\u001b[0m             read_count \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(max_read_count, count \u001b[39m-\u001b[39m i)\n\u001b[1;32m    811\u001b[0m             read_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(read_count \u001b[39m*\u001b[39m dtype\u001b[39m.\u001b[39mitemsize)\n\u001b[0;32m--> 812\u001b[0m             data \u001b[39m=\u001b[39m _read_bytes(fp, read_size, \u001b[39m\"\u001b[39;49m\u001b[39marray data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    813\u001b[0m             array[i:i\u001b[39m+\u001b[39mread_count] \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mfrombuffer(data, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m    814\u001b[0m                                                      count\u001b[39m=\u001b[39mread_count)\n\u001b[1;32m    816\u001b[0m \u001b[39mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m~/M2-Prestel-state-from-obs-ML/venv/lib/python3.9/site-packages/numpy/lib/format.py:947\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    943\u001b[0m     \u001b[39m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    944\u001b[0m     \u001b[39m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[39m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    946\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 947\u001b[0m         r \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(size \u001b[39m-\u001b[39;49m \u001b[39mlen\u001b[39;49m(data))\n\u001b[1;32m    948\u001b[0m         data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[1;32m    949\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(r) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m size:\n",
      "File \u001b[0;32m/usr/lib/python3.9/zipfile.py:922\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    921\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[0;32m--> 922\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[1;32m    923\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[1;32m    924\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m/usr/lib/python3.9/zipfile.py:1012\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1011\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_crc(data)\n\u001b[1;32m   1013\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/lib/python3.9/zipfile.py:937\u001b[0m, in \u001b[0;36mZipExtFile._update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expected_crc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[39m# No need to compute the CRC if we don't have a reference value\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc \u001b[39m=\u001b[39m crc32(newdata, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_running_crc)\n\u001b[1;32m    938\u001b[0m \u001b[39m# Check the CRC if we're at the end of the file\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expected_crc:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "x, y = load_data()\n",
    "nb_vectors = nb_vec(x)\n",
    "x_labels = list(x.keys())\n",
    "y_labels = list(y.keys())\n",
    "x_shapes = [i[0].shape for i in x.values()]\n",
    "y_shapes = [i[0].shape for i in y.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes\n",
    "\n",
    "print(f\"Number of vectors: {nb_vectors}\")\n",
    "print(f\"X shapes:\\n -\", '\\n - '.join([f\"{i}: {j}\" for i, j in zip(x_labels, x_shapes)]))\n",
    "print(f\"Y shapes:\\n -\", '\\n - '.join([f\"{i}: {j}\" for i, j in zip(y_labels, y_shapes)]))   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 10 random input vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human dataset check\n",
    "\n",
    "if is_notebook():\n",
    "\n",
    "    nb_axs = 0\n",
    "    if \"Dust map\" in x: nb_axs += 1\n",
    "    if \"CO cube\" in x: nb_axs += 1\n",
    "    if \"N2H cube\" in x: nb_axs += 1\n",
    "\n",
    "    fig, axs = plt.subplots(nb_axs, 10, figsize=(15, 5))\n",
    "    if nb_axs == 1: axs = np.array([axs])\n",
    "\n",
    "    for i in range(10):\n",
    "        vec = np.random.randint(0, nb_vectors+1)\n",
    "        nb_axis = 0\n",
    "        if \"Dust map\" in x:\n",
    "            axs[nb_axis, i].imshow(x[\"Dust map\"][vec])\n",
    "            axs[nb_axis, i].set_title(f\"Dust {vec}\")\n",
    "            nb_axis += 1\n",
    "        if \"CO cube\" in x:\n",
    "            axs[nb_axis, i].imshow(np.sum(x[\"CO cube\"][vec], axis=(-1,-2)))\n",
    "            axs[nb_axis, i].set_title(f\"CO {vec}\")\n",
    "            nb_axis += 1\n",
    "        if \"N2H cube\" in x:\n",
    "            axs[nb_axis, i].imshow(np.sum(x[\"N2H cube\"][vec], axis=(-1,-2)))\n",
    "            axs[nb_axis, i].set_title(f\"N2H+ {vec}\")\n",
    "            nb_axis += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# **Post processing**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "def normalize(x):\n",
    "\n",
    "    x_maxs = {}\n",
    "    for key, value in x.items():\n",
    "        x_maxs[key] = np.max(value)\n",
    "        x[key] /= x_maxs[key]\n",
    "\n",
    "    return x_maxs\n",
    "\n",
    "x_maxs = normalize(x)\n",
    "y_maxs = normalize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print\n",
    "\n",
    "print(\"Maximum values:\")\n",
    "pd.DataFrame.from_records([x_maxs | y_maxs, ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset\n",
    "\n",
    "def split(x, y, valid_frac=0.2, test_frac=0.1):\n",
    "\n",
    "    nb_vectors = nb_vec(x)\n",
    "    train_frac = 1 - valid_frac - test_frac\n",
    "    \n",
    "    train_x, valid_x, test_x = {}, {}, {}\n",
    "\n",
    "    for key, value in x.items():\n",
    "        train_x[key] = value[:int(nb_vectors*train_frac)]\n",
    "        valid_x[key] = value[int(nb_vectors*train_frac):int(nb_vectors*(train_frac+valid_frac))]\n",
    "        test_x[key] = value[int(nb_vectors*(train_frac+valid_frac)):]\n",
    "\n",
    "    train_y, valid_y, test_y = {}, {}, {}\n",
    "\n",
    "    for key, value in y.items():\n",
    "        train_y[key] = value[:int(nb_vectors*train_frac)]\n",
    "        valid_y[key] = value[int(nb_vectors*train_frac):int(nb_vectors*(train_frac+valid_frac))]\n",
    "        test_y[key] = value[int(nb_vectors*(train_frac+valid_frac)):]\n",
    "\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y\n",
    "\n",
    "x, y = shuffle(x, y)\n",
    "\n",
    "train_x, train_y, valid_x, valid_y, test_x, test_y = split(x, y, valid_frac, test_frac)\n",
    "\n",
    "print(f\"Train set: {nb_vec(train_x)} vectors\")\n",
    "print(f\"Valid set: {nb_vec(valid_x)} vectors\")\n",
    "print(f\"Test set: {nb_vec(test_x)} vectors\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# **Model definition**\n",
    "\n",
    "</div>\n",
    "\n",
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit the model to fit your needs\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, UpSampling2D, UpSampling3D, Reshape, Conv3DTranspose, Flatten, Concatenate, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "def get_model(sample):\n",
    "\n",
    "    # Inputs ------------------------------------------------------------------\n",
    "\n",
    "    inputs = {}\n",
    "    for key, value in sample.items():\n",
    "        inputs.update({key: Input(shape=value.shape, name=key)})\n",
    "\n",
    "    # Network -----------------------------------------------------------------\n",
    "\n",
    "    x = Conv2D(16, (8, 8), activation='relu', padding='same', name=\"Dust_map_conv_1\")(inputs[\"Dust map\"])\n",
    "    x = MaxPooling2D((4, 4), padding='same', name=\"Dust_map_max_pool_1\")(x)\n",
    "    x = Conv2D(8, (4, 4), activation='relu', padding='same', name=\"Dust_map_conv_2\")(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same', name=\"Dust_map_max_pool_2\")(x)\n",
    "    x = Flatten(name=\"Dust_map_flatting\")(x)\n",
    "\n",
    "    # x2 = Conv3D(16, (8, 8, 8), activation='relu', padding='same', name=\"CO_cube_conv_1\")(inputs[\"CO cube\"])\n",
    "    # x2 = MaxPooling3D((4, 4, 4), padding='same', name=\"CO_cube_max_pool_1\")(x2)\n",
    "    # x2 = Conv3D(8, (4, 4, 4), activation='relu', padding='same', name=\"CO_cube_conv_2\")(x2)\n",
    "    # x2 = MaxPooling3D((2, 2, 2), padding='same', name=\"CO_cube_max_pool_2\")(x2)\n",
    "    # x2 = Flatten(name=\"CO_cube_flatting\")(x2)\n",
    "\n",
    "    # x = Concatenate(name=\"Concatenation\")([x, x2])\n",
    "\n",
    "    x = Dense(4, activation='relu', name=\"Total_mass_dense\")(x)\n",
    "\n",
    "    # x_mass = Dense(128, activation='relu', name=\"Total_mass_dense\")(x)\n",
    "    total_mass = Dense(1, activation='relu', name=\"Total_mass\")(x)\n",
    "    \n",
    "    # x_temp = Dense(128, activation='relu', name=\"Max_temperature_dense\")(x)\n",
    "    max_temp = Dense(1, activation='relu', name=\"Max_temperature\")(x)\n",
    "\n",
    "    # Outputs -----------------------------------------------------------------\n",
    "\n",
    "    outputs = {\n",
    "        \"Total mass\" : total_mass,\n",
    "        \"Max temperature\" : max_temp,\n",
    "    }\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model and get summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model\n",
    "\n",
    "model = get_model(get_sample(x))\n",
    "\n",
    "def tf_pearson(y_true, y_pred):\n",
    "    return tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true)[1]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# Store and print model summary\n",
    "stringlist = []\n",
    "model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "short_model_summary = \"\\n\".join(stringlist)\n",
    "print(short_model_summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating progress bar\n",
    "\n",
    "bar = progress.Bar(epochs)\n",
    "stage=0\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global bar\n",
    "        bar(epoch, prefix = f\"Loss: {logs['loss']:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trining model\n",
    "\n",
    "bar(0)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(valid_x, valid_y), \n",
    "                    verbose=0,\n",
    "                    callbacks=[CustomCallback()],\n",
    "                    workers=10,\n",
    "                    use_multiprocessing=True)\n",
    "\n",
    "training_time = time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "\n",
    "model.save(f'{archive_path}/model0.h5')\n",
    "with open(f'{archive_path}/history.pickle', \"wb\") as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "bar(epochs)\n",
    "\n",
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show trining history\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "for key in history.history.keys():\n",
    "    if (not key.startswith('val_')) and (key.endswith('_loss')):\n",
    "        plt.plot(history.history[key], alpha=0.5, label=key.replace(\" \", \"\"))\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "for key in history.history.keys():\n",
    "    if (not key.startswith('val_')) and (key.endswith('_loss')):\n",
    "        plt.plot(history.history[key], alpha=0.5, label=key.replace(\" \", \"\"))\n",
    "plt.title('Model loss in log:log scale')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "for key in history.history.keys():\n",
    "    if (key.startswith('val_')) and (not key.endswith('_loss')):\n",
    "        plt.plot(history.history[key], alpha=0.5, label=key.replace(\" \", \"\"))\n",
    "plt.title('Model metrics')\n",
    "plt.ylabel('Metric')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "for key in history.history.keys():\n",
    "    if (key.startswith('val_')) and (not key.endswith('_loss')):\n",
    "        plt.plot(history.history[key], alpha=0.5, label=key.replace(\" \", \"\"))\n",
    "plt.title('Model metrics in log:log scale')\n",
    "plt.ylabel('Metric')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save details\n",
    "\n",
    "with open(f\"{archive_path}/model_details.json\", \"w\") as fp:\n",
    "\n",
    "    json.dump({\n",
    "        \"summary\": short_model_summary,\n",
    "        \"loss\": loss,   \n",
    "        \"optimizer\": optimizer,\n",
    "        \"metrics\": [str(i) for i in metrics],\n",
    "        \"valid_frac\": valid_frac,\n",
    "        \"test_frac\": test_frac,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"training_time\": training_time,\n",
    "        \"score\": score,\n",
    "        \"dataset_size\": nb_vectors,\n",
    "        \"path\": archive_path,\n",
    "        \"inputs\": [str(i) for i in x.keys()],\n",
    "        \"outputs\": [str(i) for i in y.keys()],\n",
    "    }, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add record to comparison file\n",
    "\n",
    "for key in y.keys():\n",
    "    file_path = f\"data/model_comparison/{key.replace(' ','_')}.yml\"\n",
    "\n",
    "    with open(file_path, \"a\") as text_file:\n",
    "            text_file.write(f\"- {archive_path}\\n\")\n",
    "\n",
    "# Detailed\n",
    "\n",
    "inputs_txt = ','.join(x_labels).replace(' ', '_')\n",
    "outputs_txt = ','.join(y_labels).replace(' ', '_')\n",
    "\n",
    "file_path = f\"data/model_comparison/detailed/{inputs_txt}---{outputs_txt}.yml\"\n",
    "\n",
    "with open(file_path, \"a\") as text_file:\n",
    "        text_file.write(f\"- {archive_path}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# **Predictions**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.randint(0, nb_vectors+1)\n",
    "x_prediction, y_expected = pick_vec(x, y, r)\n",
    "print(x_prediction.shape)\n",
    "\n",
    "y_prediction = model.predict(x_prediction)[0,0]\n",
    "print(y_prediction.shape)\n",
    "\n",
    "print(f\"Expected: {y_expected * y_maxs[0]:.2e} Msun\")\n",
    "print(f\"Predicted: {y_prediction * y_maxs[0]:.2e} Msun\")\n",
    "\n",
    "np.savez_compressed(f'{archive_path}/prediction.npz', x=x_prediction, y=y_prediction, expected=y_expected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
