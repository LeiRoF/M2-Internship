{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# **Initialisation**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 24 12:04:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   24C    P8    21W / 250W |      3MiB / 22698MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvcc: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from LRFutils import archive, progress\n",
    "from multiprocess import Pool, cpu_count\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_frac = 0.2\n",
    "test_frac  = 0.1\n",
    "dataset_path = \"data/dataset\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_path = archive.new()\n",
    "\n",
    "try:\n",
    "    ncpu = cpu_count()\n",
    "except:\n",
    "    with open(os.getenv(\"OAR_NODEFILE\"), 'r') as f:\n",
    "        ncpu = len(f.readlines())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_info():\n",
    "    return f\"CPU: {psutil.cpu_percent()}%\"\\\n",
    "        + f\", RAM: {psutil.virtual_memory().percent}%\"\\\n",
    "        + f\" ({psutil.virtual_memory().used/1024**3:.2f}GB\"\\\n",
    "        + f\"/ {psutil.virtual_memory().total/1024**3:.2f}GB)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# **Load data**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_labels(x,y):\n",
    "    \"\"\"Take a vector of a dataset and return it's properties\"\"\"\n",
    "\n",
    "    x_labels = [\n",
    "        # \"Dust Obs. Wavelenght [um]\",\n",
    "        \"Dust Map\",\n",
    "        # \"CO Velocity\",\n",
    "        # \"CO Cube\",\n",
    "        # \"N2H+ Velocity\",\n",
    "        # \"N2H Cube\"\n",
    "    ]\n",
    "    \n",
    "    y_labels = [\"Mass\"]\n",
    "\n",
    "    return x_labels, y_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file):\n",
    "\n",
    "    data = np.load(file)\n",
    "\n",
    "    x = [\n",
    "        # np.array(250), # dust observation frequency [um}\n",
    "        data[\"dust_image\"].reshape(*data[\"dust_image\"].shape, 1), # adding a channel dimension\n",
    "        # data[\"CO_v\"],\n",
    "        # data[\"CO_cube\"].reshape(*data[\"CO_cube\"].shape, 1), # adding a channel dimension\n",
    "        # data[\"N2H_v\"],\n",
    "        # data[\"N2H_cube\"].reshape(*data[\"N2H_cube\"].shape, 1) # adding a channel dimension\n",
    "    ]\n",
    "    \n",
    "    y = [np.array(data[\"mass\"]),]\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> tuple[np.ndarray, tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Do what you want int this function, as long as it returns the following:\n",
    "    - list[3D-ndarray] : input vectors\n",
    "    - list[3D-ndarray] : output vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Limit of the number of vectors to load\n",
    "    max_files = 1000\n",
    "    files = os.listdir(dataset_path)\n",
    "    nb_vectors = min(len(files), max_files)\n",
    "\n",
    "    # Load data\n",
    "    x = []\n",
    "    y = []\n",
    "    bar = progress.Bar(nb_vectors, \"Loading data\")\n",
    "    for i, file in enumerate(files):\n",
    "        if i >= nb_vectors:\n",
    "            break\n",
    "        bar(i, prefix=system_info())\n",
    "        \n",
    "        new_x, new_y = load_file(f\"{dataset_path}/{file}\")\n",
    "        x.append(new_x)\n",
    "        y.append(new_y)\n",
    "        \n",
    "    bar(nb_vectors)    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_data()\n",
    "x_labels, y_labels = data_labels(x[0], y[0])\n",
    "nb_vectors = len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 25 random dust map\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(15, 15))\n",
    "\n",
    "# sort by mass\n",
    "x_sorted = [x for _, x in sorted(zip(y, x))]\n",
    "y_sorted = sorted(y)\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        # vec = np.random.randint(0, len(x)+1)\n",
    "        vec = (i*5+j) * len(x)//25\n",
    "        axs[i, j].set_title(f\"Mass = {y_sorted[vec][0]:.2e} Msun\")\n",
    "        im = axs[i, j].imshow(x_sorted[vec][0])\n",
    "        plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 10 random complete input vector \n",
    "\n",
    "# fig, axs = plt.subplots(10, 4, figsize=(15, 15))\n",
    "\n",
    "# for i in range(10):\n",
    "#     vec = np.random.randint(0, len(x)+1)\n",
    "#     print(\"----------\")\n",
    "#     print(len(x))\n",
    "#     print(vec)\n",
    "#     print(len(x[vec]))\n",
    "#     axs[i, 0].imshow(x[vec][1])\n",
    "#     axs[i, 1].plot(x[vec][2], label=r\"CO $\\nu$\")\n",
    "#     axs[i, 1].plot(x[vec][4], label=r\"N2H+ $\\nu$\")\n",
    "#     axs[i, 1].legend()\n",
    "#     axs[i, 2].imshow(np.sum(x[vec][3], axis=(-1,-2)))\n",
    "#     axs[i, 3].imshow(np.sum(x[vec][5], axis=(-1,-2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# **Post processing**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_maxs = []\n",
    "for element in x[0]:\n",
    "    x_maxs.append(element.ravel()[0])\n",
    "\n",
    "y_maxs = []\n",
    "for element in y[0]:\n",
    "    y_maxs.append(element.ravel()[0])\n",
    "\n",
    "for vector in x:\n",
    "    for i in range(len(vector)):\n",
    "        if (value := np.max(np.abs(vector[i]))) > x_maxs[i]:\n",
    "            x_maxs[i] = value\n",
    "\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(vector)):\n",
    "        x[i][j] /= x_maxs[j]\n",
    "\n",
    "for vector in y:\n",
    "    for i in range(len(vector)):\n",
    "        if (value := np.max(np.abs(vector[i]))) > y_maxs[i]:\n",
    "            y_maxs[i] = value\n",
    "\n",
    "for i in range(len(y)):\n",
    "    for j in range(len(vector)):\n",
    "        y[i][j] /= y_maxs[j]\n",
    "\n",
    "print(\"Maximum values:\")\n",
    "df = pd.DataFrame(np.matrix(x_maxs + y_maxs))\n",
    "df.columns = x_labels + y_labels\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting datasets\n",
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 1 - valid_frac - test_frac\n",
    "\n",
    "train_x = x[:int(nb_vectors * train_frac)]\n",
    "train_y = y[:int(nb_vectors * train_frac)]\n",
    "\n",
    "df = pd.DataFrame(train_x)\n",
    "df.columns = x_labels\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_y)\n",
    "df.columns = y_labels\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = x[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "valid_y = y[int(nb_vectors * train_frac):int(nb_vectors * (train_frac + valid_frac))]\n",
    "\n",
    "df = pd.DataFrame(valid_x)\n",
    "df.columns = x_labels\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_y)\n",
    "df.columns = y_labels\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = x[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "test_y = y[int(nb_vectors * (train_frac + valid_frac)):]\n",
    "\n",
    "df = pd.DataFrame(test_x)\n",
    "df.columns = x_labels\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_y)\n",
    "df.columns = y_labels\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([i[0] for i in x])\n",
    "train_x = np.array([i[0] for i in train_x])\n",
    "valid_x = np.array([i[0] for i in valid_x])\n",
    "test_x = np.array([i[0] for i in test_x])\n",
    "y = np.array([i[0] for i in y])\n",
    "train_y = np.array([i[0] for i in train_y])\n",
    "valid_y = np.array([i[0] for i in valid_y])\n",
    "test_y = np.array([i[0] for i in test_y])\n",
    "\n",
    "print(x.shape)\n",
    "print(train_x.shape)\n",
    "print(valid_x.shape)\n",
    "print(test_x.shape)\n",
    "print(y.shape)\n",
    "print(train_y.shape)\n",
    "print(valid_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# **Model definition**\n",
    "\n",
    "</div>\n",
    "\n",
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    from keras.layers import Input, Dense, Conv2D, MaxPooling2D, MaxPooling3D, UpSampling2D, UpSampling3D, Reshape, Conv3DTranspose, Flatten\n",
    "    from keras.models import Model\n",
    "\n",
    "    # Définir la forme de l'image d'entrée\n",
    "    input = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    # x = Conv2D(8, (5, 5), activation='relu', padding='same')(input)\n",
    "    # x = MaxPooling2D((4, 4), padding='same')(x)\n",
    "    # x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D((4, 4), padding='same')(x)\n",
    "    # x = Flatten()(x)\n",
    "    # x = Dense(1024, activation='relu')(x)\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(1, activation='relu')(x)\n",
    "\n",
    "    # Modèle d'auto-encodeur\n",
    "    model = Model(input, output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model and get summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(x[0].shape)\n",
    "\n",
    "def tf_pearson(y_true, y_pred):\n",
    "    return tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true)[1]\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='SGD', metrics=['mean_squared_error'])\n",
    "\n",
    "# Store and print model summary\n",
    "stringlist = []\n",
    "model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "short_model_summary = \"\\n\".join(stringlist)\n",
    "print(short_model_summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice = input(\"Continue ? [Y/n]\")\n",
    "\n",
    "# if choice.lower() not in [\"\", \"y\", \"yes\"]:\n",
    "#     exit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "\n",
    "bar = progress.Bar(epochs)\n",
    "bar(0)\n",
    "stage=0\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global bar\n",
    "        bar(epoch, prefix = f\"MSE: {logs['mean_squared_error']:.2e}\")\n",
    "\n",
    "\n",
    "history = model.fit(train_x, train_y, epochs=10000, batch_size=50, validation_data=(valid_x, valid_y), verbose=0, callbacks=[CustomCallback()])\n",
    "model.save(f'{archive_path}/model0.h5')\n",
    "bar(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['mean_squared_error'], alpha=0.5)\n",
    "plt.plot(history.history['val_mean_squared_error'], alpha=0.5)\n",
    "plt.title('Model MSE')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], alpha=0.5)\n",
    "plt.plot(history.history['val_loss'], alpha=0.5)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], alpha=0.5)\n",
    "plt.plot(history.history['val_loss'], alpha=0.5)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Score:\", score)\n",
    "\n",
    "with open(f'{archive_path}/scores.txt', 'w') as f:\n",
    "    f.write(f'Score:    \\t{score}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "# **Predictions**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.randint(0, len(x)+1)\n",
    "x_prediction = np.array([x[r]])\n",
    "print(x_prediction.shape)\n",
    "\n",
    "y_prediction = model.predict(x_prediction)[0,0]\n",
    "print(y_prediction.shape)\n",
    "\n",
    "print(f\"Expected: {y[r] * y_maxs[0]:.2e} Msun\")\n",
    "print(f\"Predicted: {y_prediction * y_maxs[0]:.2e} Msun\")\n",
    "\n",
    "np.savez_compressed(f'{archive_path}/prediction.npz', x=x_prediction, y=y_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
