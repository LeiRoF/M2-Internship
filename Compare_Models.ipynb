{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = \"output/Max_temperature\" # file name (in the folder \"data/model_comparison\") of the comparison to be made\n",
    "consider_models = \"all\" # list of 4 digit hexa string or \"all\"\n",
    "exclude_models = [] # list of 4 digit hexa string or empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "import yaml\n",
    "from IPython.display import *\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import *\n",
    "from IPython.display import Image, display\n",
    "\n",
    "source = f\"data/model_comparison/{compare}.yml\"\n",
    "\n",
    "with open(source) as f:\n",
    "    archives = yaml.safe_load(f)\n",
    "\n",
    "models = {}\n",
    "\n",
    "print(\"Comparing models:\")\n",
    "for model_number, archive in archives.items():\n",
    "\n",
    "    try:\n",
    "\n",
    "        details = json.load(open(f'{archive}/details.json'))\n",
    "        model_id = details[\"model_id\"]\n",
    "\n",
    "        if consider_models != \"all\"\\\n",
    "                and model_id not in consider_models\\\n",
    "                or model_id in exclude_models:\n",
    "            continue\n",
    "\n",
    "        history = np.load(f'{archive}/history.npz', allow_pickle=True)\n",
    "        content = np.load(f'{archive}/predictions.npz', allow_pickle=True)\n",
    "        predictions = content[\"predictions\"].item()\n",
    "        expectations = content[\"expectations\"].item()\n",
    "\n",
    "        print(f\" - {model_number}: {archive}/details.json\")\n",
    "        print(f\"      History keys: {', '.join(list(history.keys()))}\")\n",
    "        \n",
    "        models[model_id] = {\n",
    "            \"archive\": archive,\n",
    "            \"details\": details,\n",
    "            \"history\": history,\n",
    "            \"predictions\": predictions,\n",
    "            \"expectations\": expectations\n",
    "        }\n",
    "    except:\n",
    "        print(\"Could not load model\", model_number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training evolution\n",
    "\n",
    "# Looking at how many different val metrics are available\n",
    "nb_metrics = 1\n",
    "seen_metrics = [\"loss\"]\n",
    "for model_id, model in models.items():\n",
    "    for key in model[\"history\"].keys():\n",
    "        if key.startswith(\"val_\") and not key.endswith(\"loss\") and key not in seen_metrics:\n",
    "            seen_metrics.append(key)\n",
    "            nb_metrics += 1\n",
    "\n",
    "fig, axs = plt.subplots(nb_metrics, 3, figsize=(20, 5*nb_metrics))\n",
    "\n",
    "for model_id, model in models.items():\n",
    "\n",
    "    history = model[\"history\"]\n",
    "    time_range = np.linspace(0, model[\"details\"][\"training_time\"], model[\"details\"][\"epochs\"], endpoint=True)\n",
    "\n",
    "    # Printing loss\n",
    "    for i, key in enumerate(seen_metrics):\n",
    "        if key in history:\n",
    "            for ax in axs[i]:\n",
    "                curve = history[key]\n",
    "                ax.plot(time_range, curve, alpha=0.5, label=f\"Model {model_id}\")\n",
    "\n",
    "\n",
    "for i, row in enumerate(axs):\n",
    "    for ax in row:\n",
    "        ax.set_title(seen_metrics[i])\n",
    "        ax.set_ylabel(seen_metrics[i])\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.legend()\n",
    "    row[1].set_yscale('log')\n",
    "    row[2].set_xscale('log')\n",
    "    row[2].set_yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction distributions\n",
    "\n",
    "#==============================================================================\n",
    "# Violin plots (scalar predictions)\n",
    "#==============================================================================\n",
    "\n",
    "\n",
    "def violin_plot(output, model_list, limits):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    labels = []\n",
    "    def add_label(violin, label):\n",
    "        color = violin[\"bodies\"][0].get_facecolor().flatten()\n",
    "        labels.append((mpatches.Patch(color=color), label))\n",
    "\n",
    "    i = 0\n",
    "    for key in model_list:\n",
    "        model = models[key]\n",
    "\n",
    "        expectations = np.array(model[\"expectations\"][output])  \n",
    "        predictions = np.array(model[\"predictions\"][output])\n",
    "\n",
    "        vmin = float(limits[0])\n",
    "        vmax = float(limits[1])\n",
    "        eps = float(limits[1]) - float(limits[0]) * 0.01\n",
    "\n",
    "        expectations = expectations.flatten()\n",
    "        dim_order = list(range(len(predictions.shape)))\n",
    "        dim_order.pop(1)\n",
    "        dim_order.append(1)\n",
    "        predictions = np.transpose(predictions, dim_order)\n",
    "        N = predictions.shape[-1]\n",
    "        S = predictions.size\n",
    "        predictions = predictions.reshape((S//N,N)).T\n",
    "        \n",
    "        mask = ((expectations + eps >= vmin) & (expectations - eps <= vmax)).flatten()\n",
    "        expectations = expectations[mask]\n",
    "        predictions = predictions[:,mask]\n",
    "        \n",
    "\n",
    "        emin = np.min(expectations)\n",
    "        emax = np.max(expectations)\n",
    "\n",
    "        plt.plot([emin,emax],[emin,emax], c=\"black\", linewidth=1)\n",
    "\n",
    "        violin_parts = plt.violinplot(\n",
    "            dataset=predictions[:,:],\n",
    "            positions=expectations[:], \n",
    "            widths=(np.max(expectations)-np.min(expectations))*0.05, showmeans=True, showextrema=False\n",
    "        )\n",
    "\n",
    "        add_label(violin_parts, f\"Model {key}\")\n",
    " \n",
    "    plt.title(f'Model predictions on {output}')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.xlabel('Expected')\n",
    "    plt.ticklabel_format(axis=\"both\", style='sci', scilimits=(0,0))\n",
    "    plt.grid()\n",
    "    plt.legend(*zip(*labels), loc=2)\n",
    "    plt.show()\n",
    "\n",
    "#==============================================================================\n",
    "# GUI\n",
    "#==============================================================================\n",
    "\n",
    "if compare.startswith(\"output/\"):\n",
    "    outputs = [compare.split(\"/\")[1]]\n",
    "elif compare.startswith(\"problem/\"):\n",
    "    outputs = compare.split(\"/\")[1].split(\"---\")[1].split(\",\")\n",
    "else:\n",
    "    raise ValueError(\"Invalid compare argument\")\n",
    "\n",
    "model_list_select = widgets.SelectMultiple(\n",
    "    options=list(models.keys()),\n",
    "    value=list(models.keys()),\n",
    "    description='Models',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "sample_model = models[list(models.keys())[0]]\n",
    "output_widget = Dropdown(options=outputs, value=outputs[0], description=\"Output: \")\n",
    "\n",
    "def get_possible_expectations():\n",
    "    possible_expectations = np.array([])\n",
    "    for model in models.values():\n",
    "        possible_expectations = np.concatenate((\n",
    "            possible_expectations,\n",
    "            model[\"expectations\"][output_widget.value].flatten()\n",
    "        ))\n",
    "\n",
    "    possible_expectations = list(map(lambda x: f\"{x:.2e}\", np.sort(possible_expectations).tolist()))\n",
    "    return possible_expectations\n",
    "\n",
    "expectation_slider = widgets.SelectionRangeSlider(\n",
    "    options=get_possible_expectations(),\n",
    "    index=(0, len(get_possible_expectations())-1),\n",
    "    description='Range',\n",
    "    disabled=False,\n",
    "    width='100%',\n",
    "    layout=Layout(width='100%')  \n",
    ")\n",
    "\n",
    "plot_button = widgets.Button(\n",
    "    description='Plot',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "def update_violin_plot(*args):\n",
    "    clear_output()\n",
    "    display(model_list_select)\n",
    "    if len(outputs) > 1:\n",
    "        display(output_widget)\n",
    "    display(expectation_slider)\n",
    "    display(plot_button)\n",
    "\n",
    "    violin_plot(output_widget.value, model_list_select.value, expectation_slider.value)\n",
    "\n",
    "plot_button.on_click(update_violin_plot)\n",
    "update_violin_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model and view details\n",
    "\n",
    "def print_model_details(id):\n",
    "\n",
    "    model = models[id]\n",
    "    archive = model[\"archive\"]\n",
    "    details = model[\"details\"]\n",
    "\n",
    "    display(Image(filename=f'{archive}/model.png'))\n",
    "    print(\"\\n\".join(details['summary'][-4:]))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Optimizer:\", details['optimizer'])\n",
    "    if isinstance(details['loss'], list):\n",
    "        print(\"Loss:\\n -\", \"\\n - \".join(details['loss']))\n",
    "    else:\n",
    "        print(\"Loss:\", details['loss'])\n",
    "    print(\"Metrics:\\n -\", \"\\n - \".join(details['metrics']))\n",
    "    print(\"\")\n",
    "    print(\"Epochs:\", details['epochs'])\n",
    "    print(\"Batch size:\", details['batch_size'])\n",
    "    print(\"\")\n",
    "    print(f\"Training time: {int(details['training_time']//60)}m {details['training_time']%60:.2f}s\" )\n",
    "    print(\"Score:\\n -\", \"\\n - \".join([f\"{i:.2e}\" for i in details['scores']]))\n",
    "    print(\"\")\n",
    "    # print(\"Dataset size:\", details['dataset_size'])\n",
    "    print(\"Validation fraction:\", int(details['val_frac'] * 100), \"%\")\n",
    "    print(\"Test fraction:\", int(details['test_frac'] * 100), \"%\")\n",
    "    print(\"\")\n",
    "    print(\"Path:\", archive)\n",
    "    print(\"\")\n",
    "    if \"inputs\" in details:\n",
    "        print(\"Inputs:\\n -\", \"\\n - \".join(details['inputs']))\n",
    "    if \"outputs\" in details:\n",
    "        print(\"Outputs:\\n -\", \"\\n - \".join(details['outputs']))\n",
    "\n",
    "options = []\n",
    "for model in models.values():\n",
    "    options.append(model[\"details\"][\"model_id\"])\n",
    "\n",
    "selected_model = options[0]\n",
    "\n",
    "model_widget = Dropdown(options=options, value=options[0], description=\"Model ID: \")\n",
    "\n",
    "# Define a function that updates the content of y based on what we select for x\n",
    "def update_model_widget(*args):\n",
    "    global selected_model\n",
    "    clear_output()\n",
    "    display(model_widget)\n",
    "    print_model_details(model_widget.value)\n",
    "    selected_model = models[model_widget.value]\n",
    "\n",
    "model_widget.observe(update_model_widget)\n",
    "update_model_widget()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
